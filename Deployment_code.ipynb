{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp38-cp38-macosx_10_11_x86_64.whl (165.2 MB)\n",
      "\u001b[K     |█████████████████████████████   | 149.8 MB 56 kB/s eta 0:04:348\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'tensorflow'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/http/client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/http/client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 189, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 178, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 316, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 121, in resolve\n",
      "    self._result = resolver.resolve(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 453, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 318, in resolve\n",
      "    name, crit = self._merge_into_criterion(r, parent=None)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _merge_into_criterion\n",
      "    crit = Criterion.from_requirement(self._p, requirement, parent)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 82, in from_requirement\n",
      "    if not cands:\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/resolvelib/structs.py\", line 124, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in __bool__\n",
      "    return any(self)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 38, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 167, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 300, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 144, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 226, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 311, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 457, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 480, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 230, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 108, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/network/download.py\", line 163, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/cli/progress_bars.py\", line 159, in iter\n",
      "    for x in it:\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_internal/network/utils.py\", line 64, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: Functional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7392b61d543f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoder1.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    182\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    183\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    178\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DESERIALIZATION_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_class_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   return deserialize_keras_object(\n\u001b[0m\u001b[1;32m    106\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;31m# In this case we are dealing with a Keras config dictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0m\u001b[1;32m    362\u001b[0m         config, module_objects, custom_objects, printable_module_name)\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    319\u001b[0m   \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: Functional"
     ]
    }
   ],
   "source": [
    "encoder = load_model('encoder1.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 10:51:27.684 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/rupesh/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import streamlit as stl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import joblib\n",
    "from bs4 import BeautifulSoup\n",
    "# from werkzeug import filename\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import skew, boxcox\n",
    "from joblib import dump, load\n",
    "import time\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "\n",
    "def final_fun_1(X):\n",
    "    \"The function which transforms all the raw input into predictions\"\n",
    "    try :\n",
    "        X = X.drop(['loss'], axis=1)\n",
    "    except :\n",
    "        X=X\n",
    "\n",
    "    start = time.time()\n",
    "    train_data = X.copy()\n",
    "\n",
    "    top_cat_feats = \"cat80,cat79,cat87,cat57,cat101,cat12,cat81,cat7,cat89,cat10,cat1,cat72,cat2,cat94,cat103,cat111,cat114,cat11,cat53,cat106,cat9,cat13,cat38,cat100,cat105,cat44,cat108,cat75,cat109,cat90,cat116,cat6,cat5,cat25\".split(\n",
    "        ',')\n",
    "\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "    def encode(string):\n",
    "        '''Using unicode encoding to encode the categorical variables , this encoding uses relative position of the alphabet to encode the categorical variables'''\n",
    "        r = 0\n",
    "        length = len(str(string))\n",
    "        for i in range(length):\n",
    "            # unicode of the alphabet - unicode of first letter\n",
    "            # +1 to give maintain non zero postion\n",
    "            # *26 for equating all the alphabets to a level as 26 is total number of alphabets\n",
    "            # To the power of the position of the charcode\n",
    "            r += (ord(str(string)[i]) - ord('A') + 1) * 26 ** (length - i - 1)\n",
    "        return r\n",
    "\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "    def mungeskewed(train, numeric_feats):\n",
    "        '''This function checks for skewness in the categorical features and applies box-cox transformation'''\n",
    "        ntrain = train.shape[0]\n",
    "\n",
    "        # Calculating the skewness on the entire data's features\n",
    "        skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "        # seperating the features which have higher than 0.25 skewness\n",
    "        skewed_feats = skewed_feats[skewed_feats > 0.25]\n",
    "        skewed_feats = skewed_feats.index\n",
    "\n",
    "        # Transforming all the highly skewed variables with BOX-cox\n",
    "        # Data leakage is avoided by checking the skewness on train_data only and skipping the test data\n",
    "        for feats in skewed_feats:\n",
    "            train[feats] = train[feats] + 1\n",
    "            train[feats], lam = boxcox(train[feats])\n",
    "        return train, ntrain\n",
    "\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "    numeric_feats = [x for x in train_data.columns[1:-1] if 'cont' in x]\n",
    "    categorical_feats = [x for x in train_data.columns[1:-1] if 'cat' in x]\n",
    "    train_test, ntrain = mungeskewed(train_data, numeric_feats)\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "\n",
    "    train = train_test.iloc[:ntrain, :].copy()\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "\n",
    "    mimx_scale_data = load(open('min_max_scale.pkl', 'rb'))\n",
    "\n",
    "    def min_max_scaler(data, var):\n",
    "        #     print(\"initiated\")\n",
    "        scaled_data = []\n",
    "        for i in (range(0, len(data))):\n",
    "            X_std = (float(data[var].iloc[i]) - float(mimx_scale_data[var].min())) / (\n",
    "                        float(mimx_scale_data[var].max()) - float(mimx_scale_data[var].min()))\n",
    "\n",
    "            scaled_data.append(float(X_std))\n",
    "        return scaled_data\n",
    "\n",
    "    # Referenced from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "\n",
    "    train[\"cont1\"] = np.sqrt(min_max_scaler(train, \"cont1\"))\n",
    "    train[\"cont4\"] = np.sqrt(min_max_scaler(train, \"cont4\"))\n",
    "    train[\"cont5\"] = np.sqrt(min_max_scaler(train, \"cont5\"))\n",
    "    train[\"cont8\"] = np.sqrt(min_max_scaler(train, \"cont8\"))\n",
    "    train[\"cont10\"] = np.sqrt(min_max_scaler(train, \"cont10\"))\n",
    "    train[\"cont11\"] = np.sqrt(min_max_scaler(train, \"cont11\"))\n",
    "    train[\"cont12\"] = np.sqrt(min_max_scaler(train, \"cont12\"))\n",
    "\n",
    "    train[\"cont6\"] = np.log(min_max_scaler(train, \"cont6\"))\n",
    "    train[\"cont7\"] = np.log(min_max_scaler(train, \"cont7\"))\n",
    "    train[\"cont9\"] = np.log(min_max_scaler(train, \"cont9\"))\n",
    "    train[\"cont13\"] = np.log(min_max_scaler(train, \"cont13\"))\n",
    "    train[\"cont14\"] = (np.maximum(train[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n",
    "\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "\n",
    "    #     print('Combining Columns')\n",
    "\n",
    "    for comb in (itertools.combinations(top_cat_feats, 2)):\n",
    "        feat = comb[0] + \"_\" + comb[1]\n",
    "        train[feat] = train[comb[0]] + train[comb[1]]\n",
    "        train[feat] = train[feat].apply(encode)\n",
    "\n",
    "    #     print('Encoding columns')\n",
    "    for col in (categorical_feats):\n",
    "        train[col] = train[col].apply(encode)\n",
    "\n",
    "    ss = load(open('tot_data_scale.pkl', 'rb'))\n",
    "    train[numeric_feats] = ss.fit_transform(train[numeric_feats].values)\n",
    "\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "    auto_res = load(open('auto_scaler.pkl', 'rb'))\n",
    "    train1 = train_data.drop(['id'], axis=1).copy()\n",
    "    k = []\n",
    "    #     enc_dict={}\n",
    "    #     list(auto_res.values)\n",
    "    for col in train1.select_dtypes(include=['object']).columns:\n",
    "        enc = auto_res[str(col)]\n",
    "        train1[col] = enc.transform(train1[col])\n",
    "\n",
    "    encoder = load_model('encoder1.h5', compile=False)\n",
    "\n",
    "    encoder.run_eagerly = True\n",
    "    X_train_encode = encoder.predict(train1)\n",
    "\n",
    "    train_final = np.concatenate((train.drop(['id'], axis=1), X_train_encode), axis=1)\n",
    "\n",
    "    # -*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\n",
    "    d_train = xgb.DMatrix(train_final)\n",
    "    modell = load(open('xgb_model.pkl', 'rb'))\n",
    "\n",
    "    predictions = modell.predict(d_train)\n",
    "    shift = 200\n",
    "    print(f'Time: {time.time() - start}')\n",
    "\n",
    "    return np.exp(predictions) - shift\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stl.title(\"All State Insurance Prediction\")\n",
    "stl.write(\"Input the data to return their respective predictions :\")\n",
    "\n",
    "filename = stl.text_input('Enter a file (in .txt) path:')\n",
    "\n",
    "if(stl.button('Submit filename')):\n",
    "\t\n",
    "   \n",
    "\ttry:\n",
    "\t    with open(filename) as input:\n",
    "\n",
    "\t        # stl.text(input.read())\n",
    "\t        stl.text(\"File loaded\")\n",
    "\t        \n",
    "\texcept FileNotFoundError:\n",
    "\t    stl.error('File not found.')\n",
    "\n",
    "\tdata = pd.read_csv(filename, sep=',')\n",
    "\n",
    "\n",
    "number_of_preds = stl.slider(\"Select the level\", 1, 100)\n",
    "  \n",
    "\n",
    "stl.text('Selected: {}'.format(number_of_preds ))\n",
    "\n",
    "if(stl.button('Submit number of preds')):\n",
    "\tnumber_of_preds=int(number_of_preds)\n",
    "\tdata = pd.read_csv(filename, sep=',')\n",
    "\n",
    "\t\n",
    "\tstl.text(\"The input data is :\")\n",
    "\tstl.text(data[:number_of_preds])\n",
    "\n",
    "\n",
    "\tstl.text(\"The predictions of the data are :\")\n",
    "\tstl.text(final_fun_1(data[:number_of_preds]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
